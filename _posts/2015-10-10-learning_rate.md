---
layout: post
title: "Learning rate의 중요성"
description: ""
category:
tags: [DNN]
---
![learning rate](https://wingshore.files.wordpress.com/2014/11/alpha3.png?w=662)

## Alexnet을 돌리다보니
구글넷을 돌려서 어느정도 유사한 성능이 나왔기에 이번에는 보다 많이 사용하지만 이전 모델인 alexnet을 돌려보고 있다.
구글넷을 돌리면서 아쉬웠던 점은
잘 나오는데 처음에 30 epoch까지만 모델을 걸어놨어서...... (그럼에도 이틀 이상 걸렸지만)
더 많이 돌리면 더 높은 성능이 나올 수 있을 것 같은데, 아쉬움이 많았다.
그렇다고 지금 다시 일주일짜릴 돌리자니, 다른 실험들도 궁금해서 뒤로 미뤄놨다.

다행히 alexnet은 구글넷보다는 훈련 속도가 빠르기 때문에 처음부터 100 epoch를 걸었다.그리고 하루고 지났는데,
구글넷과 비교하면 성능이 영 시원치 않게 오른다. top-5를 봐도 50%를 넘기지 못하고, top-1은 더욱 처참하고 말이다.

논문에서 보면 alexnet과 googlenet의 성능이 그렇게 크지 않았는데, 실험 중간에 확인한 바로는, 이해할 수 없었다.



## 갑자기 성능이 15% 치솟다.
그런데 갑자기 한 epoch에서 15%의 성능이 개선되었다. @.@

> 뭐지? 왜 갑자기 이러지?

확인하려고 참 많은 것들을 봤다. 데이터가 잘못된건가, 연산이 어디가 뭐 잘못된건가, 오만가지 생각이 들었는데,
보아하니 learning rate의 변화 때문이었다. `learning rate은 절대적 수치 뿐만 아니라,
epoch별 어떻게 변화를 줄것인가를 정하는 것도 중요한 요소`인데 나는 이번 실험에서 3-step으로 걸었고,
마침 step이 적용되는 그 시점의 epoch에서 성능이 퐉! 튄 것이다. 반대로 말하면 한 동안은 위의 그림처럼 특정 minima로 들어가지 못하고
주변을 서성이기만 했다는 이야기겠다.;


## learning rate이 중요하다!!
확실히 실험을 통해서 문헌으로 이야기되던 부분을 관찰할 수 있어서 요즘 재미있다. 게다가 reference 로 삼을만한 모델과 결과가 있고
이것과 비교를 하면서 실험을 돌릴 수 있으니 매우 편리하다. learning rate에 대한 부분도 드라마틱하게 올라가는 것을 실제로 확인하고 나니,
앞으로 설정할 때는 하나하나 고민을 더 한 다음에 적용해야겠다는 생각이 들었다.


## 기타.
처음에 딥러닝을 접했을 때, 모델들이 공개되어서 의아했었다. 사실 회사에서 나오는 모델이 최신인 추세에서 이것들을 왜 공개할까 싶었는데,
다 이유가 있었다. 모델도 중요하지만, 훈련을 결정하는 하이퍼파라메터에 대한 정보도 중요하기 때문인 것이다. 게다가 competition과 달리
훈련 데이터도 각자 다르게 들어갈테니, 같은 모델이라고 하더라도 다양한 성능을 가질 수 있겠다는 것을 확인하는 계기였다. (어쨌든 결과가 리포트 된 것이 있으니
그나마 성능이 많이 차이나면 뭔가 잘못된 것을 알겠지만, 그래도.. 추가적으로 고민할 부분이 많으니 말이다.)
